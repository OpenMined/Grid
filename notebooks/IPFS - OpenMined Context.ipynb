{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPFS in the Context of OpenMined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to give a quick introduction to IPFS functionalities used in [Grid](https://github.com/OpenMined/Grid), which is a Peer-to-Peer On-Demand Compute Grid. Grid is one step toward Federated Learning. You can find an excellent notebook demo [here](https://github.com/OpenMined/Grid/blob/master/notebooks/DenverMLGridDemo.ipynb) with the presentation recorded [here](https://www.youtube.com/watch?v=iYP4sYz0jho&feature=youtu.be&t=1h13m21s). \n",
    "\n",
    "In order to run this notebook, you just have to install [Grid](https://github.com/OpenMined/Grid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "from filelock import FileLock\n",
    "from datetime import datetime\n",
    "from threading import Thread\n",
    "import base64\n",
    "from bitcoin import base58\n",
    "\n",
    "from grid.ipfsapi.client import Client as IpfsClient\n",
    "\n",
    "api = IpfsClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Private - Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of OpenMined's pillars is privacy. We want to make sure users keep control over their data. A key technology to acheive this objective is [Federated Learning](https://research.googleblog.com/2017/04/federated-learning-collaborative.html). Currently, to get predictions on your device, users need to send their personal data to a central location from which the AI corporation will train the machine learning model. Finally, the AI corporation sends prediction back to the end user. With Federated Learning, the machine learning model will be trained directly on the end user's device, where their personal data is sitting. Their personal data will never be sent to a central location. It's only the gradients, information learned by the model based on the new data, that will be sent to a central location. Finally, the gradients received from the different devices will be aggregated to improve the predictions. This technique is a key ingredient to make sure your data stays private. You can find a great implementation of Federated Learning [here](https://medium.com/@mccorby/federated-learning-e79e054c33ef)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPFS (InterPlanetary File System)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[IPFS](https://ipfs.io/) protocole is at the core of OpenMined's Federated Learning implementation (Grid). With Federated Learning, you need to exchange data with several end users or data providers. This data can be an ML model or gradients, or simply messages to indicate existence of data that could improve the model, etc. The initial thought was to use blockchain technology to transfer data. However, it can be expensive and slow to tansfer data on-chain. This [article](https://medium.com/@mycoralhealth/learn-to-securely-share-files-on-the-blockchain-with-ipfs-219ee47df54c) articulates how IPFS and Blockchain can be leveraged to transfer data securely.\n",
    "\n",
    "Here are the main benefits of IPFS:\n",
    "- Each file and all of the blocks within it are given a unique fingerprint called a cryptographic hash.\n",
    "- IPFS removes duplications across the network and tracks version history for every file.\n",
    "- Each network node stores only content it is interested in, and some indexing information that helps figure out who is storing what.\n",
    "- When looking up files, you're asking the network to find nodes storing the content behind a unique hash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Data Through IPFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharing data through IPFS is extremely simple and fast. When you add a file to IPFS, it will return a hash. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharing Tensors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IPFS gives you the option to easily add binary, json, etc. files to IPFS. For example, if you want to share tensors, you just have to serialize the tensor, then add the file to IPFS using add_bytes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QmbWd4xUQzZjv35foX3uKeRTRgJ9X689vHURyECqQLUTKd'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a simple matrice\n",
    "data = np.array([[1,4],[65,8]])\n",
    "# Serialize the matrice using pickle\n",
    "data_serialized = pickle.dumps(data)\n",
    "# Add the file to IPFS using add_bytes\n",
    "tensor_hash = api.add_bytes(data_serialized)\n",
    "tensor_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the function add_bytes returns a unique identifier, which is a hash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the content of a file identified by a hash, you just have to use the function cat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4],\n",
       "       [65,  8]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retreive file from IPFS\n",
    "file_retreived_from_ipfs = api.cat('QmbWd4xUQzZjv35foX3uKeRTRgJ9X689vHURyECqQLUTKd')\n",
    "# Deserialize reteived file using pickle \n",
    "pickle.loads(file_retreived_from_ipfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharing Keras model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the simple concepts introduced above, you can even share a [Keras](https://keras.io/) model through IPFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add Keras model to IPFS after serializing it\n",
    "def keras2ipfs(api, model):\n",
    "    return api.add_bytes(serialize_keras_model(model))\n",
    "\n",
    "# Retreive the Keras model then deserialize it\n",
    "def ipfs2keras(api, model_addr):\n",
    "    return deserialize_keras_model(api.cat(model_addr))\n",
    "\n",
    "# Serialize Keras model\n",
    "def serialize_keras_model(model):\n",
    "    lock = FileLock('temp_model.h5.lock')\n",
    "    with lock:\n",
    "        model.save('temp_model.h5')\n",
    "        with open('temp_model.h5', 'rb') as f:\n",
    "            model_bin = f.read()\n",
    "            f.close()\n",
    "        return model_bin\n",
    "\n",
    "# Deserialize Keras model\n",
    "def deserialize_keras_model(model_bin):\n",
    "    lock = FileLock('temp_model2.h5.lock')\n",
    "    with lock:\n",
    "        with open('temp_model2.h5', 'wb') as g:\n",
    "            g.write(model_bin)\n",
    "            g.close()\n",
    "        model = keras.models.load_model('temp_model2.h5')\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works in action. First, we create a basic deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from grid.clients.keras import KerasClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "4/4 [==============================] - 1s 130ms/step - loss: 0.7452\n",
      "Epoch 2/3\n",
      "4/4 [==============================] - 0s 407us/step - loss: 0.7347\n",
      "Epoch 3/3\n",
      "4/4 [==============================] - 0s 645us/step - loss: 0.7255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x181d8a97b8>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "target = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_shape=(2,)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "sgd = SGD(lr=0.1)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "\n",
    "model.fit(input,target,epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the model to IPFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QmerfiYTJFJahDf8arV1Kb5Jy3Brvxc5oVEzZKKDK1YHPM'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hash = keras2ipfs(api, model)\n",
    "model_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If someone else wants to make predictions with this model, they can retreive the model using the model_hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retreive the Keras model from IPFS\n",
    "retreived_model = ipfs2keras(api, model_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4748178],\n",
       "       [0.6431086]], dtype=float32)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make prediction\n",
    "new_input = np.array([[0,0],[0,1]])\n",
    "retreived_model.predict(new_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila you made some predictions on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully these examples show you how easy it is to transfer tensors, models, gradients, etc., through IPFS. You can even share json-serializable Python dict. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qmb473ErRjqppLHK8GXfErQg9EaHghHE7BDGpBY1cvVKne'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_json = json.dumps({'model_name':'mnist', 'owner_id':'534564'})\n",
    "data_hash = api.add_json(data_json)\n",
    "data_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"model_name\": \"mnist\", \"owner_id\": \"534564\"}'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(api.cat('Qmb473ErRjqppLHK8GXfErQg9EaHghHE7BDGpBY1cvVKne'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish messages on Pubsub IPFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pubsub](https://ipfs.io/blog/25-pubsub/) on IPFS also plays a major role in [Grid](https://github.com/OpenMined/Grid). For example, with pubsub, an OpenMined Client node can ask the Worker node to train a model with certain specs (the model, number of epochs, batch size, etc.), then receive the trained model back from the Worker.\n",
    "\n",
    "Let's look at an example. We could broadcast the existence of a model with the owner_id on a channel called \"model\". To do so, you just have to use the function pubsub_pub. In the terminal, run the command **ipfs pubsub sub \"model\"**, then run the code below. You should see the message below appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object HTTPClient._request.<locals>.stream_decode at 0x181edba308>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = {'model_name':'mnist', 'owner_id':'534564'}\n",
    "\n",
    "api.pubsub_pub(topic = \"model\", \n",
    "                payload = json.dumps(message),\n",
    "                stream = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also listen to messages on the \"model\" channel. Certain portions of the message are encoded in base64, but you can decode the message using the following decode_message function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_message(encoded):\n",
    "        \"\"\"Decode message published on pubsub\"\"\"\n",
    "        if ('from' in encoded):\n",
    "            decoded = {}\n",
    "            decoded['from'] = base64.standard_b64decode(encoded['from'])\n",
    "            decoded['data'] = base64.standard_b64decode(\n",
    "                encoded['data']).decode('ascii')\n",
    "            decoded['seqno'] = base64.standard_b64decode(encoded['seqno'])\n",
    "            decoded['topicIDs'] = encoded['topicIDs']\n",
    "            decoded['encoded'] = encoded\n",
    "            return decoded\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the data from the message, we can use the handle_message function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_message(message):\n",
    "        msg = json.loads(message['data'])\n",
    "        return msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To listen to a specific channel, we just have to use the function pubsub_pub from the IPFS API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def listen_to_channel_impl(channel,handle_message):\n",
    "    \n",
    "        new_messages = api.pubsub_sub(topic=channel,stream = True)\n",
    "\n",
    "        # new_messages is a generator which will keep yield new messages until\n",
    "        # you return from the loop. If you do return from the loop, we will no\n",
    "        # longer be subscribed.\n",
    "        for m in new_messages:\n",
    "            message = decode_message(m)\n",
    "            if message is not None:\n",
    "                out = handle_message(message)\n",
    "                if out is not None:\n",
    "                    print(out)\n",
    "                    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def listen_to_channel(*args):\n",
    "        \"\"\" Listens for IPFS pubsub sub messages asynchronously.\n",
    "        This function will create the listener and call back your handler\n",
    "        function on a new thread. \"\"\"\n",
    "        t1 = Thread(target=listen_to_channel_impl, args=args)\n",
    "        t1.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we listen to the \"model\" channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listen_to_channel(\"model\", handle_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are listening to the \"model\" channel, if we publish the message below, we will receive the same message back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object HTTPClient._request.<locals>.stream_decode at 0x181edbaa98>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'mnist', 'owner_id': '534564'}\n"
     ]
    }
   ],
   "source": [
    "message = {'model_name':'mnist', 'owner_id':'534564'}\n",
    "\n",
    "api.pubsub_pub(topic = \"model\", \n",
    "                               payload=json.dumps(message),\n",
    "                               stream = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Sharing data through IPFS and publishing messages on pubsub is the backbone of Grid. If you want to contribute to Grid or learn more about IPFS, do not hesitate to reach out on [OpenMined Slack](https://openmined.slack.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:openmined2]",
   "language": "python",
   "name": "conda-env-openmined2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
