{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, this requires using PyTorch v0.3.1.  Somewhere between 0.3.1 and 0.4.0 parts of the backend were significantly rewritten, preventing us from performing the following hacks. (Likely has to do with them fusing Variable and Tensor).  That may change once their new API stabilizes.\n",
    "\n",
    "The nice thing about how this is working is that it should be general enough to work for compute, tree, and federated modes of Grid, depending on how the `receive` function works under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jason/anaconda3/envs/openmined/lib/python3.6/site-packages/h5py-2.7.1-py3.6-linux-x86_64.egg/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from grid.clients.torch import TorchClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34mUPDATE: \u001b[0mConnecting to IPFS... this can take a few seconds...\n",
      "\n",
      "\u001b[32mSUCCESS: \u001b[0mConnected!!! - My ID: QmXJMbiCqQdFCUjwy63GMUDDKCfEabJRYo2RHPjheCW8mc\n",
      "\n",
      "\u001b[34mUPDATE: \u001b[0mQuerying known workers...\n",
      "\tWORKER: /p2p-circuit/ipfs/QmXkWUybbTnfvFH8SUcrug6RGTLYTB23gSockKLxueR1vQ...\u001b[32mSUCCESS!!!\u001b[0m\n",
      "\tWORKER: /p2p-circuit/ipfs/Qmaosc64H6Y29VFCFYJzJXCX9AuRp7RCsekLmajHNVEARD...\u001b[32mSUCCESS!!!\u001b[0m\n",
      "\tWORKER: /p2p-circuit/ipfs/QmQabt3SWuDvjse9z7GAcH2BGQv4wH8bumkd4x5oXN2obX...\u001b[32mSUCCESS!!!\u001b[0m\n",
      "\tWORKER: /p2p-circuit/ipfs/Qme8SQLibzaAPQSS4GRFQCqAXqVPVknZeDLPqeePYYka8d...\u001b[32mSUCCESS!!!\u001b[0m\n",
      "\n",
      "\u001b[34mUPDATE: \u001b[0mSearching for IPFS nodes - 21 found overall - 3 are OpenMined workers          \n",
      "\n",
      "\u001b[32mSUCCESS: \u001b[0mFound 3 OpenMined nodes!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = TorchClient(verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_self = client.services['torch_service']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# service_self = client.services['torch_service']\n",
    "# def hook_float_tensor___init__(service_self):\n",
    "#     torch.FloatTensor.old___init__ = torch.FloatTensor.__init__\n",
    "#     def new___init__(self, *args, **kwargs):\n",
    "#         self.old___init__(*args, **kwargs)\n",
    "#         self = service_self.register_object(self,False)\n",
    "\n",
    "#     torch.FloatTensor.__init__ = new___init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import inspect\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import re\n",
    "from functools import wraps, partial, partialmethod\n",
    "from types import *\n",
    "import imp\n",
    "# from contextlib import contextmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_types = [torch.FloatTensor,\n",
    "                torch.DoubleTensor,\n",
    "                torch.HalfTensor,\n",
    "                torch.ByteTensor,\n",
    "                torch.CharTensor,\n",
    "                torch.ShortTensor,\n",
    "                torch.IntTensor,\n",
    "                torch.LongTensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensorvars(command):\n",
    "    args = command['args']\n",
    "    kwargs = command['kwargs']\n",
    "    arg_types = command['arg_types']\n",
    "    kwarg_types = command['kwarg_types']\n",
    "    tensorvar_args = [args[i] for i in range(len(args)) if arg_types[i] in tensor_types]\n",
    "    tensorvar_kwargs = [kwargs[i][1] for i in range(len(kwargs)) if kwarg_types[i] in tensor_types]\n",
    "    return tensorvar_args + tensorvar_kwargs\n",
    "    \n",
    "def check_tensorvars(tensorvars):\n",
    "    has_remote = any([tensorvar.is_pointer_to_remote for tensorvar in tensorvars])\n",
    "    multiple_owners = len(set([tensorvar.owner for tensorvar in tensorvars])) != 1\n",
    "    return has_remote, multiple_owners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_workers_function(worker_ids):\n",
    "    def decorate(func):\n",
    "        @wraps(func)\n",
    "        def send_to_workers(*args, **kwargs):\n",
    "            part = func(*args, **kwargs)\n",
    "            command = compile_command(part)\n",
    "            tensorvars = get_tensorvars(command)\n",
    "            has_remote, multiple_owners = check_tensorvars(tensorvars)\n",
    "            if not has_remote:\n",
    "                return part.func(*args, **kwargs)\n",
    "            elif multiple_owners:\n",
    "                raise NotImplementedError('MPC not yet implemented: Torch objects need to be on the same machine in order to compute with them.')\n",
    "            else:\n",
    "                for worker in worker_ids:\n",
    "                    print(\"Placeholder print for sending command to worker {}\".format(worker))\n",
    "                    args, kwargs = send_command(command)\n",
    "                receive_commands(worker_ids)  ## Probably needs to happen async\n",
    "                return args, kwargs\n",
    "        return send_to_workers\n",
    "    return decorate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_workers_method(worker_ids):\n",
    "    def decorate(method):\n",
    "        @wraps(method)\n",
    "        def send_to_workers(self, *args, **kwargs):\n",
    "            part = method(self, *args, **kwargs)\n",
    "            if self.is_pointer_to_remote:\n",
    "                command = compile_command(part)\n",
    "                for worker in worker_ids:\n",
    "                    print(\"Placeholder print for sending command to worker {}\".format(worker))\n",
    "                    args, kwargs = send_command(command)\n",
    "                receive_commands(worker_ids)  ## Probably needs to happen async\n",
    "                return args, kwargs\n",
    "            else:\n",
    "                result = part.func(self, *args, **kwargs)\n",
    "                if type(result) in tensor_types:\n",
    "                    my_service = self.worker.services['torch_service']\n",
    "                    result = my_service.register_object(result, False)\n",
    "                return result\n",
    "        return send_to_workers\n",
    "    return decorate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Slightly modified to remove parent class dependency\n",
    "# torch.FloatTensor.old___init__ = torch.FloatTensor.__init__\n",
    "# def hook_float_tensor___init__():\n",
    "#     def new___init__(self, tensor, owner=client.services['torch_service'], *args, **kwargs):\n",
    "#         super(torch.FloatTensor, self).__init__(*args, **kwargs)\n",
    "#         self = owner.register_object(self, False)\n",
    "\n",
    "#     torch.FloatTensor.__init__ = new___init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# service_self = client.services['torch_service']\n",
    "# def hook_float_tensor___init__(service_self):\n",
    "#     def new___init__(self, *args, **kwargs):\n",
    "#         super(torch.FloatTensor, self).__init__(*args, **kwargs)\n",
    "#         self = service_self.register_object(self,False)\n",
    "\n",
    "#     torch.FloatTensor.__init__ = new___init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def assign_workers_factory(worker_ids):\n",
    "#     def decorate(method):\n",
    "#         @wraps(method)\n",
    "#         def send_to_workers(self, *args, **kwargs):\n",
    "#             part = method(self, *args, **kwargs)\n",
    "#             command = compile_command(part)\n",
    "#             for worker in worker_ids:\n",
    "#                 print(\"Placeholder print for sending command to worker {}\".format(worker))\n",
    "#                 args, kwargs = send_command(command)\n",
    "#             receive_commands(worker_ids)  ## Probably needs to happen async\n",
    "#             return old_init(*args, **kwargs)\n",
    "#         return send_to_workers\n",
    "#     return decorate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_self = client.services['torch_service']\n",
    "def hook_tensor___init__(service_self, tensor_type):\n",
    "    def new___init__(self, tensor, *args, **kwargs):\n",
    "        super(tensor_type, self).__init__(*args, **kwargs)\n",
    "        self = service_self.register_object(self,False)\n",
    "\n",
    "    tensor_type.__init__ = new___init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_tensor___repr__(service_self, tensor_type):\n",
    "        def __repr__(self):\n",
    "            if(service_self.worker.id == self.owner):\n",
    "                return self.old__repr__()\n",
    "            else:\n",
    "                return \"[ {} - Location:{} ]\".format(tensor_type, self.owner)\n",
    "\n",
    "        # if haven't reserved the actual __repr__ function - reserve it now\n",
    "        try:\n",
    "            tensor_type.old__repr__\n",
    "        except:\n",
    "            tensor_type.old__repr__ = tensor_type.__repr__\n",
    "            \n",
    "\n",
    "        tensor_type.__repr__ = __repr__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_func_args(func):\n",
    "    @wraps(func)\n",
    "    def pass_args(*args, **kwargs):\n",
    "        return partial(func, *args, **kwargs)\n",
    "    return pass_args\n",
    "\n",
    "def pass_method_args(method):\n",
    "    @wraps(method)\n",
    "    def pass_args(*args, **kwargs):\n",
    "        return partialmethod(method, *args, **kwargs)\n",
    "    return pass_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_command(command):\n",
    "    print(command['command'])\n",
    "    print([type(arg) for arg in command['args']])\n",
    "    print([type(pair) for pair in command['kwargs']])\n",
    "    print('===========')\n",
    "    print()\n",
    "    return command['args'], command['kwargs']\n",
    "\n",
    "def receive_commands(worker_ids):\n",
    "    print('Placeholder print for receiving commands from workers in the following list')\n",
    "    print(worker_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_command(partial_func):\n",
    "    func = partial_func.func\n",
    "    args = partial_func.args\n",
    "    kwargs = partial_func.keywords\n",
    "    command = {}\n",
    "    command['command'] = func.__name__\n",
    "    command['command_type'] = type(func)\n",
    "    command['args'] = args\n",
    "    command['kwargs'] = kwargs\n",
    "    command['arg_types'] = [type(x) for x in args]\n",
    "    command['kwarg_types'] = [type(kwargs[x]) for x in kwargs]\n",
    "    return command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.22 s, sys: 23.1 ms, total: 1.24 s\n",
      "Wall time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for x in range(100000):\n",
    "    y = torch.FloatTensor([[2,2],[2,2]])\n",
    "    z = torch.FloatTensor([[1,1],[1,1]])\n",
    "    res = y.add(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hooking <class 'torch.FloatTensor'>\n",
      "==============\n",
      "__add__\n",
      "__and__\n",
      "__array__\n",
      "__array_wrap__\n",
      "__bool__\n",
      "__class__  skipped\n",
      "__deepcopy__\n",
      "__delattr__  skipped\n",
      "__delitem__\n",
      "__dict__  skipped\n",
      "__dir__  skipped\n",
      "__div__\n",
      "__doc__  skipped\n",
      "__eq__  skipped\n",
      "__float__\n",
      "__format__  skipped\n",
      "__ge__  skipped\n",
      "__getattribute__  skipped\n",
      "__getitem__\n",
      "__getstate__\n",
      "__gt__  skipped\n",
      "__hash__  skipped\n",
      "__iadd__\n",
      "__iand__\n",
      "__idiv__\n",
      "__ilshift__\n",
      "__imul__\n",
      "__init__  skipped\n",
      "__init_subclass__  skipped\n",
      "__int__\n",
      "__invert__\n",
      "__ior__\n",
      "__ipow__\n",
      "__irshift__\n",
      "__isub__\n",
      "__iter__\n",
      "__itruediv__\n",
      "__ixor__\n",
      "__le__  skipped\n",
      "__len__\n",
      "__long__\n",
      "__lshift__\n",
      "__lt__  skipped\n",
      "__matmul__\n",
      "__mod__\n",
      "__module__  skipped\n",
      "__mul__\n",
      "__ne__  skipped\n",
      "__neg__\n",
      "__new__  skipped\n",
      "__nonzero__\n",
      "__or__\n",
      "__pow__\n",
      "__radd__\n",
      "__rdiv__\n",
      "__reduce__  skipped\n",
      "__reduce_ex__  skipped\n",
      "__repr__  skipped\n",
      "__rmul__\n",
      "__rpow__\n",
      "__rshift__\n",
      "__rsub__\n",
      "__rtruediv__\n",
      "__setattr__  skipped\n",
      "__setitem__\n",
      "__setstate__\n",
      "__sizeof__  skipped\n",
      "__str__  skipped\n",
      "__sub__\n",
      "__subclasshook__  skipped\n",
      "__truediv__\n",
      "__weakref__  skipped\n",
      "__xor__\n",
      "_advanced_index_add\n",
      "_advanced_index_select\n",
      "_cdata  skipped\n",
      "_check_advanced_indexing\n",
      "_new_with_metadata_file  skipped\n",
      "_set_index\n",
      "_sparse_mask\n",
      "_torch  skipped\n",
      "_write_metadata\n",
      "abs\n",
      "abs_\n",
      "acos\n",
      "acos_\n",
      "add\n",
      "add_\n",
      "addbmm\n",
      "addbmm_\n",
      "addcdiv\n",
      "addcdiv_\n",
      "addcmul\n",
      "addcmul_\n",
      "addmm\n",
      "addmm_\n",
      "addmv\n",
      "addmv_\n",
      "addr\n",
      "addr_\n",
      "apply_\n",
      "asin\n",
      "asin_\n",
      "atan\n",
      "atan2\n",
      "atan2_\n",
      "atan_\n",
      "baddbmm\n",
      "baddbmm_\n",
      "bernoulli\n",
      "bernoulli_\n",
      "bmm\n",
      "btrifact\n",
      "btrisolve\n",
      "byte\n",
      "cauchy_\n",
      "ceil\n",
      "ceil_\n",
      "char\n",
      "chunk\n",
      "clamp\n",
      "clamp_\n",
      "clone\n",
      "contiguous\n",
      "copy_\n",
      "cos\n",
      "cos_\n",
      "cosh\n",
      "cosh_\n",
      "cpu\n",
      "cross\n",
      "cuda\n",
      "cumprod\n",
      "cumsum\n",
      "data  skipped\n",
      "data_ptr\n",
      "de  skipped\n",
      "diag\n",
      "dim\n",
      "dist\n",
      "div\n",
      "div_\n",
      "dot\n",
      "double\n",
      "eig\n",
      "element_size\n",
      "eq\n",
      "eq_\n",
      "equal\n",
      "erf\n",
      "erf_\n",
      "erfinv\n",
      "erfinv_\n",
      "exp\n",
      "exp_\n",
      "expand\n",
      "expand_as\n",
      "exponential_\n",
      "fill_\n",
      "float\n",
      "floor\n",
      "floor_\n",
      "fmod\n",
      "fmod_\n",
      "frac\n",
      "frac_\n",
      "gather\n",
      "ge\n",
      "ge_\n",
      "gels\n",
      "geometric_\n",
      "geqrf\n",
      "ger\n",
      "gesv\n",
      "get  skipped\n",
      "gt\n",
      "gt_\n",
      "half\n",
      "histc\n",
      "index\n",
      "index_add_\n",
      "index_copy_\n",
      "index_fill_\n",
      "index_select\n",
      "int\n",
      "inverse\n",
      "is_contiguous\n",
      "is_cuda  skipped\n",
      "is_pinned\n",
      "is_same_size\n",
      "is_set_to\n",
      "is_shared\n",
      "is_signed\n",
      "is_sparse  skipped\n",
      "kthvalue\n",
      "le\n",
      "le_\n",
      "lerp\n",
      "lerp_\n",
      "lgamma\n",
      "lgamma_\n",
      "log\n",
      "log1p\n",
      "log1p_\n",
      "log_\n",
      "log_normal_\n",
      "long\n",
      "lt\n",
      "lt_\n",
      "map2_\n",
      "map_\n",
      "masked_copy_\n",
      "masked_fill_\n",
      "masked_scatter_\n",
      "masked_select\n",
      "matmul\n",
      "max\n",
      "mean\n",
      "median\n",
      "min\n",
      "mm\n",
      "mode\n",
      "mul\n",
      "mul_\n",
      "multinomial\n",
      "mv\n",
      "narrow\n",
      "ndimension  skipped\n",
      "ne\n",
      "ne_\n",
      "neg\n",
      "neg_\n",
      "nelement  skipped\n",
      "new\n",
      "nonzero\n",
      "norm\n",
      "normal_\n",
      "numel  skipped\n",
      "numpy\n",
      "old__repr__  skipped\n",
      "orgqr\n",
      "ormqr\n",
      "permute\n",
      "pin_memory\n",
      "potrf\n",
      "potri\n",
      "potrs\n",
      "pow\n",
      "pow_\n",
      "process_command  skipped\n",
      "prod\n",
      "pstrf\n",
      "put_\n",
      "qr\n",
      "random_\n",
      "reciprocal\n",
      "reciprocal_\n",
      "remainder\n",
      "remainder_\n",
      "renorm\n",
      "renorm_\n",
      "repeat\n",
      "resize_\n",
      "resize_as_\n",
      "round\n",
      "round_\n",
      "rsqrt\n",
      "rsqrt_\n",
      "scatter_\n",
      "scatter_add_\n",
      "select\n",
      "send  skipped\n",
      "ser  skipped\n",
      "set_\n",
      "shape  skipped\n",
      "share_memory_\n",
      "short\n",
      "sigmoid\n",
      "sigmoid_\n",
      "sign\n",
      "sign_\n",
      "sin\n",
      "sin_\n",
      "sinh\n",
      "sinh_\n",
      "size  skipped\n",
      "sort\n",
      "split\n",
      "sqrt\n",
      "sqrt_\n",
      "squeeze\n",
      "squeeze_\n",
      "std\n",
      "storage\n",
      "storage_offset\n",
      "storage_type  skipped\n",
      "stride\n",
      "sub\n",
      "sub_\n",
      "sum\n",
      "svd\n",
      "symeig\n",
      "t\n",
      "t_\n",
      "take\n",
      "tan\n",
      "tan_\n",
      "tanh\n",
      "tanh_\n",
      "tolist\n",
      "topk\n",
      "trace\n",
      "transpose\n",
      "transpose_\n",
      "tril\n",
      "tril_\n",
      "triu\n",
      "triu_\n",
      "trtrs\n",
      "trunc\n",
      "trunc_\n",
      "type\n",
      "type_as\n",
      "unfold\n",
      "uniform_\n",
      "unsqueeze\n",
      "unsqueeze_\n",
      "var\n",
      "view\n",
      "view_as\n",
      "zero_\n",
      "\n",
      "Hooking <class 'torch.DoubleTensor'>\n",
      "==============\n",
      "__add__\n",
      "__and__\n",
      "__array__\n",
      "__array_wrap__\n",
      "__bool__\n",
      "__class__  skipped\n",
      "__deepcopy__\n",
      "__delattr__  skipped\n",
      "__delitem__\n",
      "__dict__  skipped\n",
      "__dir__  skipped\n",
      "__div__\n",
      "__doc__  skipped\n",
      "__eq__  skipped\n",
      "__float__\n",
      "__format__  skipped\n",
      "__ge__  skipped\n",
      "__getattribute__  skipped\n",
      "__getitem__\n",
      "__getstate__\n",
      "__gt__  skipped\n",
      "__hash__  skipped\n",
      "__iadd__\n",
      "__iand__\n",
      "__idiv__\n",
      "__ilshift__\n",
      "__imul__\n",
      "__init__  skipped\n",
      "__init_subclass__  skipped\n",
      "__int__\n",
      "__invert__\n",
      "__ior__\n",
      "__ipow__\n",
      "__irshift__\n",
      "__isub__\n",
      "__iter__\n",
      "__itruediv__\n",
      "__ixor__\n",
      "__le__  skipped\n",
      "__len__\n",
      "__long__\n",
      "__lshift__\n",
      "__lt__  skipped\n",
      "__matmul__\n",
      "__mod__\n",
      "__module__  skipped\n",
      "__mul__\n",
      "__ne__  skipped\n",
      "__neg__\n",
      "__new__  skipped\n",
      "__nonzero__\n",
      "__or__\n",
      "__pow__\n",
      "__radd__\n",
      "__rdiv__\n",
      "__reduce__  skipped\n",
      "__reduce_ex__  skipped\n",
      "__repr__  skipped\n",
      "__rmul__\n",
      "__rpow__\n",
      "__rshift__\n",
      "__rsub__\n",
      "__rtruediv__\n",
      "__setattr__  skipped\n",
      "__setitem__\n",
      "__setstate__\n",
      "__sizeof__  skipped\n",
      "__str__  skipped\n",
      "__sub__\n",
      "__subclasshook__  skipped\n",
      "__truediv__\n",
      "__weakref__  skipped\n",
      "__xor__\n",
      "_advanced_index_add\n",
      "_advanced_index_select\n",
      "_cdata  skipped\n",
      "_check_advanced_indexing\n",
      "_new_with_metadata_file  skipped\n",
      "_set_index\n",
      "_sparse_mask\n",
      "_torch  skipped\n",
      "_write_metadata\n",
      "abs\n",
      "abs_\n",
      "acos\n",
      "acos_\n",
      "add\n",
      "add_\n",
      "addbmm\n",
      "addbmm_\n",
      "addcdiv\n",
      "addcdiv_\n",
      "addcmul\n",
      "addcmul_\n",
      "addmm\n",
      "addmm_\n",
      "addmv\n",
      "addmv_\n",
      "addr\n",
      "addr_\n",
      "apply_\n",
      "asin\n",
      "asin_\n",
      "atan\n",
      "atan2\n",
      "atan2_\n",
      "atan_\n",
      "baddbmm\n",
      "baddbmm_\n",
      "bernoulli\n",
      "bernoulli_\n",
      "bmm\n",
      "btrifact\n",
      "btrisolve\n",
      "byte\n",
      "cauchy_\n",
      "ceil\n",
      "ceil_\n",
      "char\n",
      "chunk\n",
      "clamp\n",
      "clamp_\n",
      "clone\n",
      "contiguous\n",
      "copy_\n",
      "cos\n",
      "cos_\n",
      "cosh\n",
      "cosh_\n",
      "cpu\n",
      "cross\n",
      "cuda\n",
      "cumprod\n",
      "cumsum\n",
      "data  skipped\n",
      "data_ptr\n",
      "diag\n",
      "dim\n",
      "dist\n",
      "div\n",
      "div_\n",
      "dot\n",
      "double\n",
      "eig\n",
      "element_size\n",
      "eq\n",
      "eq_\n",
      "equal\n",
      "erf\n",
      "erf_\n",
      "erfinv\n",
      "erfinv_\n",
      "exp\n",
      "exp_\n",
      "expand\n",
      "expand_as\n",
      "exponential_\n",
      "fill_\n",
      "float\n",
      "floor\n",
      "floor_\n",
      "fmod\n",
      "fmod_\n",
      "frac\n",
      "frac_\n",
      "gather\n",
      "ge\n",
      "ge_\n",
      "gels\n",
      "geometric_\n",
      "geqrf\n",
      "ger\n",
      "gesv\n",
      "gt\n",
      "gt_\n",
      "half\n",
      "histc\n",
      "index\n",
      "index_add_\n",
      "index_copy_\n",
      "index_fill_\n",
      "index_select\n",
      "int\n",
      "inverse\n",
      "is_contiguous\n",
      "is_cuda  skipped\n",
      "is_pinned\n",
      "is_same_size\n",
      "is_set_to\n",
      "is_shared\n",
      "is_signed\n",
      "is_sparse  skipped\n",
      "kthvalue\n",
      "le\n",
      "le_\n",
      "lerp\n",
      "lerp_\n",
      "lgamma\n",
      "lgamma_\n",
      "log\n",
      "log1p\n",
      "log1p_\n",
      "log_\n",
      "log_normal_\n",
      "long\n",
      "lt\n",
      "lt_\n",
      "map2_\n",
      "map_\n",
      "masked_copy_\n",
      "masked_fill_\n",
      "masked_scatter_\n",
      "masked_select\n",
      "matmul\n",
      "max\n",
      "mean\n",
      "median\n",
      "min\n",
      "mm\n",
      "mode\n",
      "mul\n",
      "mul_\n",
      "multinomial\n",
      "mv\n",
      "narrow\n",
      "ndimension  skipped\n",
      "ne\n",
      "ne_\n",
      "neg\n",
      "neg_\n",
      "nelement  skipped\n",
      "new\n",
      "nonzero\n",
      "norm\n",
      "normal_\n",
      "numel  skipped\n",
      "numpy\n",
      "old__repr__  skipped\n",
      "orgqr\n",
      "ormqr\n",
      "permute\n",
      "pin_memory\n",
      "potrf\n",
      "potri\n",
      "potrs\n",
      "pow\n",
      "pow_\n",
      "prod\n",
      "pstrf\n",
      "put_\n",
      "qr\n",
      "random_\n",
      "reciprocal\n",
      "reciprocal_\n",
      "remainder\n",
      "remainder_\n",
      "renorm\n",
      "renorm_\n",
      "repeat\n",
      "resize_\n",
      "resize_as_\n",
      "round\n",
      "round_\n",
      "rsqrt\n",
      "rsqrt_\n",
      "scatter_\n",
      "scatter_add_\n",
      "select\n",
      "set_\n",
      "shape  skipped\n",
      "share_memory_\n",
      "short\n",
      "sigmoid\n",
      "sigmoid_\n",
      "sign\n",
      "sign_\n",
      "sin\n",
      "sin_\n",
      "sinh\n",
      "sinh_\n",
      "size  skipped\n",
      "sort\n",
      "split\n",
      "sqrt\n",
      "sqrt_\n",
      "squeeze\n",
      "squeeze_\n",
      "std\n",
      "storage\n",
      "storage_offset\n",
      "storage_type  skipped\n",
      "stride\n",
      "sub\n",
      "sub_\n",
      "sum\n",
      "svd\n",
      "symeig\n",
      "t\n",
      "t_\n",
      "take\n",
      "tan\n",
      "tan_\n",
      "tanh\n",
      "tanh_\n",
      "tolist\n",
      "topk\n",
      "trace\n",
      "transpose\n",
      "transpose_\n",
      "tril\n",
      "tril_\n",
      "triu\n",
      "triu_\n",
      "trtrs\n",
      "trunc\n",
      "trunc_\n",
      "type\n",
      "type_as\n",
      "unfold\n",
      "uniform_\n",
      "unsqueeze\n",
      "unsqueeze_\n",
      "var\n",
      "view\n",
      "view_as\n",
      "zero_\n",
      "\n",
      "Hooking <class 'torch.HalfTensor'>\n",
      "==============\n",
      "__add__\n",
      "__array__\n",
      "__array_wrap__\n",
      "__bool__\n",
      "__class__  skipped\n",
      "__deepcopy__\n",
      "__delattr__  skipped\n",
      "__delitem__\n",
      "__dict__  skipped\n",
      "__dir__  skipped\n",
      "__div__\n",
      "__doc__  skipped\n",
      "__eq__  skipped\n",
      "__float__\n",
      "__format__  skipped\n",
      "__ge__  skipped\n",
      "__getattribute__  skipped\n",
      "__getitem__\n",
      "__getstate__\n",
      "__gt__  skipped\n",
      "__hash__  skipped\n",
      "__iadd__\n",
      "__idiv__\n",
      "__imul__\n",
      "__init__  skipped\n",
      "__init_subclass__  skipped\n",
      "__int__\n",
      "__invert__\n",
      "__ipow__\n",
      "__isub__\n",
      "__iter__\n",
      "__itruediv__\n",
      "__le__  skipped\n",
      "__len__\n",
      "__long__\n",
      "__lt__  skipped\n",
      "__matmul__\n",
      "__mod__\n",
      "__module__  skipped\n",
      "__mul__\n",
      "__ne__  skipped\n",
      "__neg__\n",
      "__new__  skipped\n",
      "__nonzero__\n",
      "__pow__\n",
      "__radd__\n",
      "__rdiv__\n",
      "__reduce__  skipped\n",
      "__reduce_ex__  skipped\n",
      "__repr__  skipped\n",
      "__rmul__\n",
      "__rpow__\n",
      "__rsub__\n",
      "__rtruediv__\n",
      "__setattr__  skipped\n",
      "__setitem__\n",
      "__setstate__\n",
      "__sizeof__  skipped\n",
      "__str__  skipped\n",
      "__sub__\n",
      "__subclasshook__  skipped\n",
      "__truediv__\n",
      "__weakref__  skipped\n",
      "_cdata  skipped\n",
      "_torch  skipped\n",
      "apply_\n",
      "byte\n",
      "char\n",
      "chunk\n",
      "copy_\n",
      "cpu\n",
      "cuda\n",
      "data  skipped\n",
      "data_ptr\n",
      "dim\n",
      "double\n",
      "element_size\n",
      "expand_as\n",
      "float\n",
      "half\n",
      "int\n",
      "is_contiguous\n",
      "is_cuda  skipped\n",
      "is_pinned\n",
      "is_same_size\n",
      "is_set_to\n",
      "is_shared\n",
      "is_signed\n",
      "is_sparse  skipped\n",
      "long\n",
      "map2_\n",
      "map_\n",
      "masked_copy_\n",
      "matmul\n",
      "narrow\n",
      "ndimension  skipped\n",
      "nelement  skipped\n",
      "new\n",
      "numel  skipped\n",
      "old__repr__  skipped\n",
      "permute\n",
      "pin_memory\n",
      "repeat\n",
      "resize_\n",
      "select\n",
      "set_\n",
      "shape  skipped\n",
      "share_memory_\n",
      "short\n",
      "size  skipped\n",
      "split\n",
      "squeeze\n",
      "squeeze_\n",
      "storage\n",
      "storage_offset\n",
      "storage_type  skipped\n",
      "stride\n",
      "tolist\n",
      "transpose\n",
      "transpose_\n",
      "type\n",
      "type_as\n",
      "unfold\n",
      "unsqueeze\n",
      "unsqueeze_\n",
      "view_as\n",
      "\n",
      "Hooking <class 'torch.ByteTensor'>\n",
      "==============\n",
      "__add__\n",
      "__and__\n",
      "__array__\n",
      "__array_wrap__\n",
      "__bool__\n",
      "__class__  skipped\n",
      "__deepcopy__\n",
      "__delattr__  skipped\n",
      "__delitem__\n",
      "__dict__  skipped\n",
      "__dir__  skipped\n",
      "__div__\n",
      "__doc__  skipped\n",
      "__eq__  skipped\n",
      "__float__\n",
      "__format__  skipped\n",
      "__ge__  skipped\n",
      "__getattribute__  skipped\n",
      "__getitem__\n",
      "__getstate__\n",
      "__gt__  skipped\n",
      "__hash__  skipped\n",
      "__iadd__\n",
      "__iand__\n",
      "__idiv__\n",
      "__ilshift__\n",
      "__imul__\n",
      "__init__  skipped\n",
      "__init_subclass__  skipped\n",
      "__int__\n",
      "__invert__\n",
      "__ior__\n",
      "__ipow__\n",
      "__irshift__\n",
      "__isub__\n",
      "__iter__\n",
      "__itruediv__\n",
      "__ixor__\n",
      "__le__  skipped\n",
      "__len__\n",
      "__long__\n",
      "__lshift__\n",
      "__lt__  skipped\n",
      "__matmul__\n",
      "__mod__\n",
      "__module__  skipped\n",
      "__mul__\n",
      "__ne__  skipped\n",
      "__neg__\n",
      "__new__  skipped\n",
      "__nonzero__\n",
      "__or__\n",
      "__pow__\n",
      "__radd__\n",
      "__rdiv__\n",
      "__reduce__  skipped\n",
      "__reduce_ex__  skipped\n",
      "__repr__  skipped\n",
      "__rmul__\n",
      "__rpow__\n",
      "__rshift__\n",
      "__rsub__\n",
      "__rtruediv__\n",
      "__setattr__  skipped\n",
      "__setitem__\n",
      "__setstate__\n",
      "__sizeof__  skipped\n",
      "__str__  skipped\n",
      "__sub__\n",
      "__subclasshook__  skipped\n",
      "__truediv__\n",
      "__weakref__  skipped\n",
      "__xor__\n",
      "_advanced_index_add\n",
      "_advanced_index_select\n",
      "_cdata  skipped\n",
      "_check_advanced_indexing\n",
      "_new_with_metadata_file  skipped\n",
      "_set_index\n",
      "_sparse_mask\n",
      "_torch  skipped\n",
      "_write_metadata\n",
      "add\n",
      "add_\n",
      "addbmm\n",
      "addbmm_\n",
      "addcdiv\n",
      "addcdiv_\n",
      "addcmul\n",
      "addcmul_\n",
      "addmm\n",
      "addmm_\n",
      "addmv\n",
      "addmv_\n",
      "addr\n",
      "addr_\n",
      "all\n",
      "any\n",
      "apply_\n",
      "baddbmm\n",
      "baddbmm_\n",
      "bernoulli_\n",
      "bmm\n",
      "byte\n",
      "char\n",
      "chunk\n",
      "clamp\n",
      "clamp_\n",
      "clone\n",
      "contiguous\n",
      "copy_\n",
      "cpu\n",
      "cross\n",
      "cuda\n",
      "cumprod\n",
      "cumsum\n",
      "data  skipped\n",
      "data_ptr\n",
      "diag\n",
      "dim\n",
      "div\n",
      "div_\n",
      "dot\n",
      "double\n",
      "element_size\n",
      "eq\n",
      "eq_\n",
      "equal\n",
      "expand\n",
      "expand_as\n",
      "fill_\n",
      "float\n",
      "fmod\n",
      "fmod_\n",
      "gather\n",
      "ge\n",
      "ge_\n",
      "geometric_\n",
      "ger\n",
      "gt\n",
      "gt_\n",
      "half\n",
      "index\n",
      "index_add_\n",
      "index_copy_\n",
      "index_fill_\n",
      "index_select\n",
      "int\n",
      "is_contiguous\n",
      "is_cuda  skipped\n",
      "is_pinned\n",
      "is_same_size\n",
      "is_set_to\n",
      "is_shared\n",
      "is_signed\n",
      "is_sparse  skipped\n",
      "kthvalue\n",
      "le\n",
      "le_\n",
      "long\n",
      "lt\n",
      "lt_\n",
      "map2_\n",
      "map_\n",
      "masked_copy_\n",
      "masked_fill_\n",
      "masked_scatter_\n",
      "masked_select\n",
      "matmul\n",
      "max\n",
      "median\n",
      "min\n",
      "mm\n",
      "mode\n",
      "mul\n",
      "mul_\n",
      "mv\n",
      "narrow\n",
      "ndimension  skipped\n",
      "ne\n",
      "ne_\n",
      "nelement  skipped\n",
      "new\n",
      "nonzero\n",
      "numel  skipped\n",
      "numpy\n",
      "old__repr__  skipped\n",
      "permute\n",
      "pin_memory\n",
      "prod\n",
      "put_\n",
      "random_\n",
      "remainder\n",
      "remainder_\n",
      "repeat\n",
      "resize_\n",
      "resize_as_\n",
      "scatter_\n",
      "scatter_add_\n",
      "select\n",
      "set_\n",
      "shape  skipped\n",
      "share_memory_\n",
      "short\n",
      "sign\n",
      "sign_\n",
      "size  skipped\n",
      "sort\n",
      "split\n",
      "squeeze\n",
      "squeeze_\n",
      "storage\n",
      "storage_offset\n",
      "storage_type  skipped\n",
      "stride\n",
      "sub\n",
      "sub_\n",
      "sum\n",
      "t\n",
      "t_\n",
      "take\n",
      "tolist\n",
      "topk\n",
      "trace\n",
      "transpose\n",
      "transpose_\n",
      "tril\n",
      "tril_\n",
      "triu\n",
      "triu_\n",
      "type\n",
      "type_as\n",
      "unfold\n",
      "unsqueeze\n",
      "unsqueeze_\n",
      "view\n",
      "view_as\n",
      "zero_\n",
      "\n",
      "Hooking <class 'torch.CharTensor'>\n",
      "==============\n",
      "__add__\n",
      "__and__\n",
      "__array__\n",
      "__array_wrap__\n",
      "__bool__\n",
      "__class__  skipped\n",
      "__deepcopy__\n",
      "__delattr__  skipped\n",
      "__delitem__\n",
      "__dict__  skipped\n",
      "__dir__  skipped\n",
      "__div__\n",
      "__doc__  skipped\n",
      "__eq__  skipped\n",
      "__float__\n",
      "__format__  skipped\n",
      "__ge__  skipped\n",
      "__getattribute__  skipped\n",
      "__getitem__\n",
      "__getstate__\n",
      "__gt__  skipped\n",
      "__hash__  skipped\n",
      "__iadd__\n",
      "__iand__\n",
      "__idiv__\n",
      "__ilshift__\n",
      "__imul__\n",
      "__init__  skipped\n",
      "__init_subclass__  skipped\n",
      "__int__\n",
      "__invert__\n",
      "__ior__\n",
      "__ipow__\n",
      "__irshift__\n",
      "__isub__\n",
      "__iter__\n",
      "__itruediv__\n",
      "__ixor__\n",
      "__le__  skipped\n",
      "__len__\n",
      "__long__\n",
      "__lshift__\n",
      "__lt__  skipped\n",
      "__matmul__\n",
      "__mod__\n",
      "__module__  skipped\n",
      "__mul__\n",
      "__ne__  skipped\n",
      "__neg__\n",
      "__new__  skipped\n",
      "__nonzero__\n",
      "__or__\n",
      "__pow__\n",
      "__radd__\n",
      "__rdiv__\n",
      "__reduce__  skipped\n",
      "__reduce_ex__  skipped\n",
      "__repr__  skipped\n",
      "__rmul__\n",
      "__rpow__\n",
      "__rshift__\n",
      "__rsub__\n",
      "__rtruediv__\n",
      "__setattr__  skipped\n",
      "__setitem__\n",
      "__setstate__\n",
      "__sizeof__  skipped\n",
      "__str__  skipped\n",
      "__sub__\n",
      "__subclasshook__  skipped\n",
      "__truediv__\n",
      "__weakref__  skipped\n",
      "__xor__\n",
      "_advanced_index_add\n",
      "_advanced_index_select\n",
      "_cdata  skipped\n",
      "_check_advanced_indexing\n",
      "_new_with_metadata_file  skipped\n",
      "_set_index\n",
      "_sparse_mask\n",
      "_torch  skipped\n",
      "_write_metadata\n",
      "add\n",
      "add_\n",
      "addbmm\n",
      "addbmm_\n",
      "addcdiv\n",
      "addcdiv_\n",
      "addcmul\n",
      "addcmul_\n",
      "addmm\n",
      "addmm_\n",
      "addmv\n",
      "addmv_\n",
      "addr\n",
      "addr_\n",
      "apply_\n",
      "baddbmm\n",
      "baddbmm_\n",
      "bernoulli_\n",
      "bmm\n",
      "byte\n",
      "char\n",
      "chunk\n",
      "clamp\n",
      "clamp_\n",
      "clone\n",
      "contiguous\n",
      "copy_\n",
      "cpu\n",
      "cross\n",
      "cuda\n",
      "cumprod\n",
      "cumsum\n",
      "data  skipped\n",
      "data_ptr\n",
      "diag\n",
      "dim\n",
      "div\n",
      "div_\n",
      "dot\n",
      "double\n",
      "element_size\n",
      "eq\n",
      "eq_\n",
      "equal\n",
      "expand\n",
      "expand_as\n",
      "fill_\n",
      "float\n",
      "fmod\n",
      "fmod_\n",
      "gather\n",
      "ge\n",
      "ge_\n",
      "geometric_\n",
      "ger\n",
      "gt\n",
      "gt_\n",
      "half\n",
      "index\n",
      "index_add_\n",
      "index_copy_\n",
      "index_fill_\n",
      "index_select\n",
      "int\n",
      "is_contiguous\n",
      "is_cuda  skipped\n",
      "is_pinned\n",
      "is_same_size\n",
      "is_set_to\n",
      "is_shared\n",
      "is_signed\n",
      "is_sparse  skipped\n",
      "kthvalue\n",
      "le\n",
      "le_\n",
      "long\n",
      "lt\n",
      "lt_\n",
      "map2_\n",
      "map_\n",
      "masked_copy_\n",
      "masked_fill_\n",
      "masked_scatter_\n",
      "masked_select\n",
      "matmul\n",
      "max\n",
      "median\n",
      "min\n",
      "mm\n",
      "mode\n",
      "mul\n",
      "mul_\n",
      "mv\n",
      "narrow\n",
      "ndimension  skipped\n",
      "ne\n",
      "ne_\n",
      "nelement  skipped\n",
      "new\n",
      "nonzero\n",
      "numel  skipped\n",
      "numpy\n",
      "old__repr__  skipped\n",
      "permute\n",
      "pin_memory\n",
      "prod\n",
      "put_\n",
      "random_\n",
      "remainder\n",
      "remainder_\n",
      "repeat\n",
      "resize_\n",
      "resize_as_\n",
      "scatter_\n",
      "scatter_add_\n",
      "select\n",
      "set_\n",
      "shape  skipped\n",
      "share_memory_\n",
      "short\n",
      "sign\n",
      "sign_\n",
      "size  skipped\n",
      "sort\n",
      "split\n",
      "squeeze\n",
      "squeeze_\n",
      "storage\n",
      "storage_offset\n",
      "storage_type  skipped\n",
      "stride\n",
      "sub\n",
      "sub_\n",
      "sum\n",
      "t\n",
      "t_\n",
      "take\n",
      "tolist\n",
      "topk\n",
      "trace\n",
      "transpose\n",
      "transpose_\n",
      "tril\n",
      "tril_\n",
      "triu\n",
      "triu_\n",
      "type\n",
      "type_as\n",
      "unfold\n",
      "unsqueeze\n",
      "unsqueeze_\n",
      "view\n",
      "view_as\n",
      "zero_\n",
      "\n",
      "Hooking <class 'torch.ShortTensor'>\n",
      "==============\n",
      "__add__\n",
      "__and__\n",
      "__array__\n",
      "__array_wrap__\n",
      "__bool__\n",
      "__class__  skipped\n",
      "__deepcopy__\n",
      "__delattr__  skipped\n",
      "__delitem__\n",
      "__dict__  skipped\n",
      "__dir__  skipped\n",
      "__div__\n",
      "__doc__  skipped\n",
      "__eq__  skipped\n",
      "__float__\n",
      "__format__  skipped\n",
      "__ge__  skipped\n",
      "__getattribute__  skipped\n",
      "__getitem__\n",
      "__getstate__\n",
      "__gt__  skipped\n",
      "__hash__  skipped\n",
      "__iadd__\n",
      "__iand__\n",
      "__idiv__\n",
      "__ilshift__\n",
      "__imul__\n",
      "__init__  skipped\n",
      "__init_subclass__  skipped\n",
      "__int__\n",
      "__invert__\n",
      "__ior__\n",
      "__ipow__\n",
      "__irshift__\n",
      "__isub__\n",
      "__iter__\n",
      "__itruediv__\n",
      "__ixor__\n",
      "__le__  skipped\n",
      "__len__\n",
      "__long__\n",
      "__lshift__\n",
      "__lt__  skipped\n",
      "__matmul__\n",
      "__mod__\n",
      "__module__  skipped\n",
      "__mul__\n",
      "__ne__  skipped\n",
      "__neg__\n",
      "__new__  skipped\n",
      "__nonzero__\n",
      "__or__\n",
      "__pow__\n",
      "__radd__\n",
      "__rdiv__\n",
      "__reduce__  skipped\n",
      "__reduce_ex__  skipped\n",
      "__repr__  skipped\n",
      "__rmul__\n",
      "__rpow__\n",
      "__rshift__\n",
      "__rsub__\n",
      "__rtruediv__\n",
      "__setattr__  skipped\n",
      "__setitem__\n",
      "__setstate__\n",
      "__sizeof__  skipped\n",
      "__str__  skipped\n",
      "__sub__\n",
      "__subclasshook__  skipped\n",
      "__truediv__\n",
      "__weakref__  skipped\n",
      "__xor__\n",
      "_advanced_index_add\n",
      "_advanced_index_select\n",
      "_cdata  skipped\n",
      "_check_advanced_indexing\n",
      "_new_with_metadata_file  skipped\n",
      "_set_index\n",
      "_sparse_mask\n",
      "_torch  skipped\n",
      "_write_metadata\n",
      "abs\n",
      "abs_\n",
      "add\n",
      "add_\n",
      "addbmm\n",
      "addbmm_\n",
      "addcdiv\n",
      "addcdiv_\n",
      "addcmul\n",
      "addcmul_\n",
      "addmm\n",
      "addmm_\n",
      "addmv\n",
      "addmv_\n",
      "addr\n",
      "addr_\n",
      "apply_\n",
      "baddbmm\n",
      "baddbmm_\n",
      "bernoulli_\n",
      "bmm\n",
      "byte\n",
      "char\n",
      "chunk\n",
      "clamp\n",
      "clamp_\n",
      "clone\n",
      "contiguous\n",
      "copy_\n",
      "cpu\n",
      "cross\n",
      "cuda\n",
      "cumprod\n",
      "cumsum\n",
      "data  skipped\n",
      "data_ptr\n",
      "diag\n",
      "dim\n",
      "div\n",
      "div_\n",
      "dot\n",
      "double\n",
      "element_size\n",
      "eq\n",
      "eq_\n",
      "equal\n",
      "expand\n",
      "expand_as\n",
      "fill_\n",
      "float\n",
      "fmod\n",
      "fmod_\n",
      "gather\n",
      "ge\n",
      "ge_\n",
      "geometric_\n",
      "ger\n",
      "gt\n",
      "gt_\n",
      "half\n",
      "index\n",
      "index_add_\n",
      "index_copy_\n",
      "index_fill_\n",
      "index_select\n",
      "int\n",
      "is_contiguous\n",
      "is_cuda  skipped\n",
      "is_pinned\n",
      "is_same_size\n",
      "is_set_to\n",
      "is_shared\n",
      "is_signed\n",
      "is_sparse  skipped\n",
      "kthvalue\n",
      "le\n",
      "le_\n",
      "long\n",
      "lt\n",
      "lt_\n",
      "map2_\n",
      "map_\n",
      "masked_copy_\n",
      "masked_fill_\n",
      "masked_scatter_\n",
      "masked_select\n",
      "matmul\n",
      "max\n",
      "median\n",
      "min\n",
      "mm\n",
      "mode\n",
      "mul\n",
      "mul_\n",
      "mv\n",
      "narrow\n",
      "ndimension  skipped\n",
      "ne\n",
      "ne_\n",
      "neg\n",
      "neg_\n",
      "nelement  skipped\n",
      "new\n",
      "nonzero\n",
      "numel  skipped\n",
      "numpy\n",
      "old__repr__  skipped\n",
      "permute\n",
      "pin_memory\n",
      "prod\n",
      "put_\n",
      "random_\n",
      "remainder\n",
      "remainder_\n",
      "repeat\n",
      "resize_\n",
      "resize_as_\n",
      "scatter_\n",
      "scatter_add_\n",
      "select\n",
      "set_\n",
      "shape  skipped\n",
      "share_memory_\n",
      "short\n",
      "sign\n",
      "sign_\n",
      "size  skipped\n",
      "sort\n",
      "split\n",
      "squeeze\n",
      "squeeze_\n",
      "storage\n",
      "storage_offset\n",
      "storage_type  skipped\n",
      "stride\n",
      "sub\n",
      "sub_\n",
      "sum\n",
      "t\n",
      "t_\n",
      "take\n",
      "tolist\n",
      "topk\n",
      "trace\n",
      "transpose\n",
      "transpose_\n",
      "tril\n",
      "tril_\n",
      "triu\n",
      "triu_\n",
      "type\n",
      "type_as\n",
      "unfold\n",
      "unsqueeze\n",
      "unsqueeze_\n",
      "view\n",
      "view_as\n",
      "zero_\n",
      "\n",
      "Hooking <class 'torch.IntTensor'>\n",
      "==============\n",
      "__add__\n",
      "__and__\n",
      "__array__\n",
      "__array_wrap__\n",
      "__bool__\n",
      "__class__  skipped\n",
      "__deepcopy__\n",
      "__delattr__  skipped\n",
      "__delitem__\n",
      "__dict__  skipped\n",
      "__dir__  skipped\n",
      "__div__\n",
      "__doc__  skipped\n",
      "__eq__  skipped\n",
      "__float__\n",
      "__format__  skipped\n",
      "__ge__  skipped\n",
      "__getattribute__  skipped\n",
      "__getitem__\n",
      "__getstate__\n",
      "__gt__  skipped\n",
      "__hash__  skipped\n",
      "__iadd__\n",
      "__iand__\n",
      "__idiv__\n",
      "__ilshift__\n",
      "__imul__\n",
      "__init__  skipped\n",
      "__init_subclass__  skipped\n",
      "__int__\n",
      "__invert__\n",
      "__ior__\n",
      "__ipow__\n",
      "__irshift__\n",
      "__isub__\n",
      "__iter__\n",
      "__itruediv__\n",
      "__ixor__\n",
      "__le__  skipped\n",
      "__len__\n",
      "__long__\n",
      "__lshift__\n",
      "__lt__  skipped\n",
      "__matmul__\n",
      "__mod__\n",
      "__module__  skipped\n",
      "__mul__\n",
      "__ne__  skipped\n",
      "__neg__\n",
      "__new__  skipped\n",
      "__nonzero__\n",
      "__or__\n",
      "__pow__\n",
      "__radd__\n",
      "__rdiv__\n",
      "__reduce__  skipped\n",
      "__reduce_ex__  skipped\n",
      "__repr__  skipped\n",
      "__rmul__\n",
      "__rpow__\n",
      "__rshift__\n",
      "__rsub__\n",
      "__rtruediv__\n",
      "__setattr__  skipped\n",
      "__setitem__\n",
      "__setstate__\n",
      "__sizeof__  skipped\n",
      "__str__  skipped\n",
      "__sub__\n",
      "__subclasshook__  skipped\n",
      "__truediv__\n",
      "__weakref__  skipped\n",
      "__xor__\n",
      "_advanced_index_add\n",
      "_advanced_index_select\n",
      "_cdata  skipped\n",
      "_check_advanced_indexing\n",
      "_new_with_metadata_file  skipped\n",
      "_set_index\n",
      "_sparse_mask\n",
      "_torch  skipped\n",
      "_write_metadata\n",
      "abs\n",
      "abs_\n",
      "add\n",
      "add_\n",
      "addbmm\n",
      "addbmm_\n",
      "addcdiv\n",
      "addcdiv_\n",
      "addcmul\n",
      "addcmul_\n",
      "addmm\n",
      "addmm_\n",
      "addmv\n",
      "addmv_\n",
      "addr\n",
      "addr_\n",
      "apply_\n",
      "baddbmm\n",
      "baddbmm_\n",
      "bernoulli_\n",
      "bmm\n",
      "byte\n",
      "char\n",
      "chunk\n",
      "clamp\n",
      "clamp_\n",
      "clone\n",
      "contiguous\n",
      "copy_\n",
      "cpu\n",
      "cross\n",
      "cuda\n",
      "cumprod\n",
      "cumsum\n",
      "data  skipped\n",
      "data_ptr\n",
      "diag\n",
      "dim\n",
      "div\n",
      "div_\n",
      "dot\n",
      "double\n",
      "element_size\n",
      "eq\n",
      "eq_\n",
      "equal\n",
      "expand\n",
      "expand_as\n",
      "fill_\n",
      "float\n",
      "fmod\n",
      "fmod_\n",
      "gather\n",
      "ge\n",
      "ge_\n",
      "geometric_\n",
      "ger\n",
      "gt\n",
      "gt_\n",
      "half\n",
      "index\n",
      "index_add_\n",
      "index_copy_\n",
      "index_fill_\n",
      "index_select\n",
      "int\n",
      "is_contiguous\n",
      "is_cuda  skipped\n",
      "is_pinned\n",
      "is_same_size\n",
      "is_set_to\n",
      "is_shared\n",
      "is_signed\n",
      "is_sparse  skipped\n",
      "kthvalue\n",
      "le\n",
      "le_\n",
      "long\n",
      "lt\n",
      "lt_\n",
      "map2_\n",
      "map_\n",
      "masked_copy_\n",
      "masked_fill_\n",
      "masked_scatter_\n",
      "masked_select\n",
      "matmul\n",
      "max\n",
      "median\n",
      "min\n",
      "mm\n",
      "mode\n",
      "mul\n",
      "mul_\n",
      "mv\n",
      "narrow\n",
      "ndimension  skipped\n",
      "ne\n",
      "ne_\n",
      "neg\n",
      "neg_\n",
      "nelement  skipped\n",
      "new\n",
      "nonzero\n",
      "numel  skipped\n",
      "numpy\n",
      "old__repr__  skipped\n",
      "permute\n",
      "pin_memory\n",
      "prod\n",
      "put_\n",
      "random_\n",
      "remainder\n",
      "remainder_\n",
      "repeat\n",
      "resize_\n",
      "resize_as_\n",
      "scatter_\n",
      "scatter_add_\n",
      "select\n",
      "set_\n",
      "shape  skipped\n",
      "share_memory_\n",
      "short\n",
      "sign\n",
      "sign_\n",
      "size  skipped\n",
      "sort\n",
      "split\n",
      "squeeze\n",
      "squeeze_\n",
      "storage\n",
      "storage_offset\n",
      "storage_type  skipped\n",
      "stride\n",
      "sub\n",
      "sub_\n",
      "sum\n",
      "t\n",
      "t_\n",
      "take\n",
      "tolist\n",
      "topk\n",
      "trace\n",
      "transpose\n",
      "transpose_\n",
      "tril\n",
      "tril_\n",
      "triu\n",
      "triu_\n",
      "type\n",
      "type_as\n",
      "unfold\n",
      "unsqueeze\n",
      "unsqueeze_\n",
      "view\n",
      "view_as\n",
      "zero_\n",
      "\n",
      "Hooking <class 'torch.LongTensor'>\n",
      "==============\n",
      "__add__\n",
      "__and__\n",
      "__array__\n",
      "__array_wrap__\n",
      "__bool__\n",
      "__class__  skipped\n",
      "__deepcopy__\n",
      "__delattr__  skipped\n",
      "__delitem__\n",
      "__dict__  skipped\n",
      "__dir__  skipped\n",
      "__div__\n",
      "__doc__  skipped\n",
      "__eq__  skipped\n",
      "__float__\n",
      "__format__  skipped\n",
      "__ge__  skipped\n",
      "__getattribute__  skipped\n",
      "__getitem__\n",
      "__getstate__\n",
      "__gt__  skipped\n",
      "__hash__  skipped\n",
      "__iadd__\n",
      "__iand__\n",
      "__idiv__\n",
      "__ilshift__\n",
      "__imul__\n",
      "__init__  skipped\n",
      "__init_subclass__  skipped\n",
      "__int__\n",
      "__invert__\n",
      "__ior__\n",
      "__ipow__\n",
      "__irshift__\n",
      "__isub__\n",
      "__iter__\n",
      "__itruediv__\n",
      "__ixor__\n",
      "__le__  skipped\n",
      "__len__\n",
      "__long__\n",
      "__lshift__\n",
      "__lt__  skipped\n",
      "__matmul__\n",
      "__mod__\n",
      "__module__  skipped\n",
      "__mul__\n",
      "__ne__  skipped\n",
      "__neg__\n",
      "__new__  skipped\n",
      "__nonzero__\n",
      "__or__\n",
      "__pow__\n",
      "__radd__\n",
      "__rdiv__\n",
      "__reduce__  skipped\n",
      "__reduce_ex__  skipped\n",
      "__repr__  skipped\n",
      "__rmul__\n",
      "__rpow__\n",
      "__rshift__\n",
      "__rsub__\n",
      "__rtruediv__\n",
      "__setattr__  skipped\n",
      "__setitem__\n",
      "__setstate__\n",
      "__sizeof__  skipped\n",
      "__str__  skipped\n",
      "__sub__\n",
      "__subclasshook__  skipped\n",
      "__truediv__\n",
      "__weakref__  skipped\n",
      "__xor__\n",
      "_advanced_index_add\n",
      "_advanced_index_select\n",
      "_cdata  skipped\n",
      "_check_advanced_indexing\n",
      "_new_with_metadata_file  skipped\n",
      "_set_index\n",
      "_sparse_mask\n",
      "_torch  skipped\n",
      "_write_metadata\n",
      "abs\n",
      "abs_\n",
      "add\n",
      "add_\n",
      "addbmm\n",
      "addbmm_\n",
      "addcdiv\n",
      "addcdiv_\n",
      "addcmul\n",
      "addcmul_\n",
      "addmm\n",
      "addmm_\n",
      "addmv\n",
      "addmv_\n",
      "addr\n",
      "addr_\n",
      "apply_\n",
      "baddbmm\n",
      "baddbmm_\n",
      "bernoulli_\n",
      "bmm\n",
      "byte\n",
      "char\n",
      "chunk\n",
      "clamp\n",
      "clamp_\n",
      "clone\n",
      "contiguous\n",
      "copy_\n",
      "cpu\n",
      "cross\n",
      "cuda\n",
      "cumprod\n",
      "cumsum\n",
      "data  skipped\n",
      "data_ptr\n",
      "diag\n",
      "dim\n",
      "div\n",
      "div_\n",
      "dot\n",
      "double\n",
      "element_size\n",
      "eq\n",
      "eq_\n",
      "equal\n",
      "expand\n",
      "expand_as\n",
      "fill_\n",
      "float\n",
      "fmod\n",
      "fmod_\n",
      "gather\n",
      "ge\n",
      "ge_\n",
      "geometric_\n",
      "ger\n",
      "gt\n",
      "gt_\n",
      "half\n",
      "index\n",
      "index_add_\n",
      "index_copy_\n",
      "index_fill_\n",
      "index_select\n",
      "int\n",
      "is_contiguous\n",
      "is_cuda  skipped\n",
      "is_pinned\n",
      "is_same_size\n",
      "is_set_to\n",
      "is_shared\n",
      "is_signed\n",
      "is_sparse  skipped\n",
      "kthvalue\n",
      "le\n",
      "le_\n",
      "long\n",
      "lt\n",
      "lt_\n",
      "map2_\n",
      "map_\n",
      "masked_copy_\n",
      "masked_fill_\n",
      "masked_scatter_\n",
      "masked_select\n",
      "matmul\n",
      "max\n",
      "median\n",
      "min\n",
      "mm\n",
      "mode\n",
      "mul\n",
      "mul_\n",
      "mv\n",
      "narrow\n",
      "ndimension  skipped\n",
      "ne\n",
      "ne_\n",
      "neg\n",
      "neg_\n",
      "nelement  skipped\n",
      "new\n",
      "nonzero\n",
      "numel  skipped\n",
      "numpy\n",
      "old__repr__  skipped\n",
      "permute\n",
      "pin_memory\n",
      "prod\n",
      "put_\n",
      "random_\n",
      "remainder\n",
      "remainder_\n",
      "repeat\n",
      "resize_\n",
      "resize_as_\n",
      "scatter_\n",
      "scatter_add_\n",
      "select\n",
      "set_\n",
      "shape  skipped\n",
      "share_memory_\n",
      "short\n",
      "sign\n",
      "sign_\n",
      "size  skipped\n",
      "sort\n",
      "split\n",
      "squeeze\n",
      "squeeze_\n",
      "storage\n",
      "storage_offset\n",
      "storage_type  skipped\n",
      "stride\n",
      "sub\n",
      "sub_\n",
      "sum\n",
      "t\n",
      "t_\n",
      "take\n",
      "tolist\n",
      "topk\n",
      "trace\n",
      "transpose\n",
      "transpose_\n",
      "tril\n",
      "tril_\n",
      "triu\n",
      "triu_\n",
      "type\n",
      "type_as\n",
      "unfold\n",
      "unsqueeze\n",
      "unsqueeze_\n",
      "view\n",
      "view_as\n",
      "zero_\n",
      "\n",
      "CPU times: user 124 ms, sys: 49.9 ms, total: 174 ms\n",
      "Wall time: 156 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for attr in dir(torch):\n",
    "    if attr == 'typename':\n",
    "        continue\n",
    "    if type(torch.__getattribute__(attr)) in [FunctionType, BuiltinFunctionType]:\n",
    "        torch.__setattr__(attr, assign_workers_function(['A1','B1', 'B2'])(pass_func_args(torch.__getattribute__(attr))))\n",
    "\n",
    "exclude = ['ndimension', 'nelement', 'size','numel', 'ser', 'de']\n",
    "for tensor_type in tensor_types:\n",
    "    print('Hooking {}'.format(tensor_type))\n",
    "    print('==============')\n",
    "    if tensor_type is not torch.FloatTensor:\n",
    "        hook_tensor___init__(service_self, tensor_type)\n",
    "        hook_tensor___repr__(service_self, tensor_type)\n",
    "    for attr in dir(tensor_type):\n",
    "        lit = getattr(tensor_type, attr)\n",
    "        is_desc = inspect.ismethoddescriptor(lit)\n",
    "        is_func = type(lit)==FunctionType\n",
    "        is_mappingproxy = attr == '__dict__'\n",
    "        try:\n",
    "            is_service_func = 'TorchService' in lit.__qualname__\n",
    "        except:\n",
    "            is_service_func = False\n",
    "        is_base = attr in dir(object)\n",
    "        is_old = re.match('old*', attr) is not None\n",
    "        if attr in exclude:\n",
    "            print(attr,' skipped')\n",
    "            continue\n",
    "        if (is_desc or (is_func and not is_service_func)) and not is_base and not is_old:\n",
    "            print(attr)\n",
    "            setattr(tensor_type, 'old_{}'.format(attr), lit)\n",
    "            setattr(tensor_type, attr, assign_workers_method(['A1','B1', 'B2'])(pass_method_args(lit)))\n",
    "        else:\n",
    "            print(attr, ' skipped')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.04 s, sys: 446 ms, total: 2.49 s\n",
      "Wall time: 2.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for x in range(100000):\n",
    "    y = torch.FloatTensor([[2,2],[2,2]])\n",
    "    z = torch.FloatTensor([[1,1],[1,1]])\n",
    "    res = y.add(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = y.add(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "664525414\n"
     ]
    }
   ],
   "source": [
    "print(x.is_pointer_to_remote)\n",
    "print(x.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 3  3\n",
       " 3  3\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  0\n",
       " 0  0\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0  0\n",
      " 0  0\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case when tensor isn't local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.is_pointer_to_remote = True\n",
    "x.owner = 'other_guy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder print for sending command to worker A1\n",
      "normal_\n",
      "[<class 'torch.FloatTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B1\n",
      "normal_\n",
      "[<class 'torch.FloatTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B2\n",
      "normal_\n",
      "[<class 'torch.FloatTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for receiving commands from workers in the following list\n",
      "['A1', 'B1', 'B2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(([ torch.FloatTensor - Location:other_guy ],), {})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder print for sending command to worker A1\n",
      "uniform_\n",
      "[<class 'torch.FloatTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B1\n",
      "uniform_\n",
      "[<class 'torch.FloatTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B2\n",
      "uniform_\n",
      "[<class 'torch.FloatTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for receiving commands from workers in the following list\n",
      "['A1', 'B1', 'B2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(([ torch.FloatTensor - Location:other_guy ],), {})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.uniform_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder print for sending command to worker A1\n",
      "add\n",
      "[<class 'torch.FloatTensor'>, <class 'torch.FloatTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B1\n",
      "add\n",
      "[<class 'torch.FloatTensor'>, <class 'torch.FloatTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B2\n",
      "add\n",
      "[<class 'torch.FloatTensor'>, <class 'torch.FloatTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for receiving commands from workers in the following list\n",
      "['A1', 'B1', 'B2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(([ torch.FloatTensor - Location:other_guy ],\n",
       "  [ torch.FloatTensor - Location:other_guy ]),\n",
       " {})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "booped!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    torch.add(x,y) # This should throw an error, since their attributes say they're not on the same machine.\n",
    "except NotImplementedError:\n",
    "    print('booped!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DoubleTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.DoubleTensor([[2,2],[2,2]])\n",
    "z = torch.DoubleTensor(([[1,1],[1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = y.add(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "5174648574\n"
     ]
    }
   ],
   "source": [
    "print(x.is_pointer_to_remote)\n",
    "print(x.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 3  3\n",
       " 3  3\n",
       "[torch.DoubleTensor of size 2x2]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  0\n",
       " 0  0\n",
       "[torch.DoubleTensor of size 2x2]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0  0\n",
      " 0  0\n",
      "[torch.DoubleTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case when tensor isn't local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.is_pointer_to_remote = True\n",
    "x.owner = 'other_guy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder print for sending command to worker A1\n",
      "normal_\n",
      "[<class 'torch.DoubleTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B1\n",
      "normal_\n",
      "[<class 'torch.DoubleTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B2\n",
      "normal_\n",
      "[<class 'torch.DoubleTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for receiving commands from workers in the following list\n",
      "['A1', 'B1', 'B2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(([ <class 'torch.DoubleTensor'> - Location:other_guy ],), {})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder print for sending command to worker A1\n",
      "uniform_\n",
      "[<class 'torch.DoubleTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B1\n",
      "uniform_\n",
      "[<class 'torch.DoubleTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B2\n",
      "uniform_\n",
      "[<class 'torch.DoubleTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for receiving commands from workers in the following list\n",
      "['A1', 'B1', 'B2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(([ <class 'torch.DoubleTensor'> - Location:other_guy ],), {})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.uniform_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder print for sending command to worker A1\n",
      "add\n",
      "[<class 'torch.DoubleTensor'>, <class 'torch.DoubleTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B1\n",
      "add\n",
      "[<class 'torch.DoubleTensor'>, <class 'torch.DoubleTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B2\n",
      "add\n",
      "[<class 'torch.DoubleTensor'>, <class 'torch.DoubleTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for receiving commands from workers in the following list\n",
      "['A1', 'B1', 'B2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(([ <class 'torch.DoubleTensor'> - Location:other_guy ],\n",
       "  [ <class 'torch.DoubleTensor'> - Location:other_guy ]),\n",
       " {})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HalfTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.HalfTensor([[2,2],[2,2]])\n",
    "z = torch.HalfTensor(([[1,1],[1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2  2\n",
       " 2  2\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case when tensor isn't local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.is_pointer_to_remote = True\n",
    "y.owner = 'other_guy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder print for sending command to worker A1\n",
      "float\n",
      "[<class 'torch.HalfTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B1\n",
      "float\n",
      "[<class 'torch.HalfTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B2\n",
      "float\n",
      "[<class 'torch.HalfTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for receiving commands from workers in the following list\n",
      "['A1', 'B1', 'B2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(([ <class 'torch.HalfTensor'> - Location:other_guy ],), {})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder print for sending command to worker A1\n",
      "add\n",
      "[<class 'torch.HalfTensor'>, <class 'torch.HalfTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B1\n",
      "add\n",
      "[<class 'torch.HalfTensor'>, <class 'torch.HalfTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B2\n",
      "add\n",
      "[<class 'torch.HalfTensor'>, <class 'torch.HalfTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for receiving commands from workers in the following list\n",
      "['A1', 'B1', 'B2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(([ <class 'torch.HalfTensor'> - Location:other_guy ],\n",
       "  [ <class 'torch.HalfTensor'> - Location:other_guy ]),\n",
       " {})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(y, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.HalfTensor([[2,2],[2,2]])\n",
    "b = torch.HalfTensor(([[1,1],[1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HalfTensor is weird\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    torch.add(a,b)\n",
    "except:\n",
    "    print('HalfTensor is weird')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.DoubleTensor([[1,2],[3,4]])\n",
    "z = torch.DoubleTensor(([[1,1],[1,1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = y.add(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "7411756871\n"
     ]
    }
   ],
   "source": [
    "print(x.is_pointer_to_remote)\n",
    "print(x.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2  3\n",
       " 4  5\n",
       "[torch.DoubleTensor of size 2x2]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 2  4\n",
       " 3  5\n",
       "[torch.DoubleTensor of size 2x2]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  0\n",
       " 0  0\n",
       "[torch.DoubleTensor of size 2x2]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0  0\n",
      " 0  0\n",
      "[torch.DoubleTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0  0\n",
       " 0  0\n",
       "[torch.DoubleTensor of size 2x2]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case when tensor isn't local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.is_pointer_to_remote = True\n",
    "x.owner = 'other_guy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x.normal_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x.uniform_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder print for sending command to worker A1\n",
      "t\n",
      "[<class 'torch.DoubleTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B1\n",
      "t\n",
      "[<class 'torch.DoubleTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B2\n",
      "t\n",
      "[<class 'torch.DoubleTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for receiving commands from workers in the following list\n",
      "['A1', 'B1', 'B2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(([ <class 'torch.DoubleTensor'> - Location:other_guy ],), {})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder print for sending command to worker A1\n",
      "add\n",
      "[<class 'torch.DoubleTensor'>, <class 'torch.DoubleTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B1\n",
      "add\n",
      "[<class 'torch.DoubleTensor'>, <class 'torch.DoubleTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for sending command to worker B2\n",
      "add\n",
      "[<class 'torch.DoubleTensor'>, <class 'torch.DoubleTensor'>]\n",
      "[]\n",
      "===========\n",
      "\n",
      "Placeholder print for receiving commands from workers in the following list\n",
      "['A1', 'B1', 'B2']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(([ <class 'torch.DoubleTensor'> - Location:other_guy ],\n",
       "  [ <class 'torch.DoubleTensor'> - Location:other_guy ]),\n",
       " {})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
