{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Federated Learning with Grid Nodes </h1>\n",
    "This notebook was made based on <a href=\"https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/Part%2010%20-%20Federated%20Learning%20with%20Secure%20Aggregation.ipynb\">Part 10: Federated Learning with Encrypted Gradient Aggregation</a> tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import dependencies</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import grid as gr\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import syft as sy\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Setup config</h2>\n",
    "Define Config parameters, init hook, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "class Parser:\n",
    "    \"\"\"Parameters for training\"\"\"\n",
    "    def __init__(self):\n",
    "        self.epochs = 1\n",
    "        self.lr = 0.001\n",
    "        self.test_batch_size = 8\n",
    "        self.batch_size = 8\n",
    "        self.log_interval = 10\n",
    "        self.seed = 1\n",
    "    \n",
    "args = Parser()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/boston_housing.pickle','rb') as f:\n",
    "    ((X, y), (X_test, y_test)) = pickle.load(f)\n",
    "\n",
    "X = torch.from_numpy(X).float()\n",
    "y = torch.from_numpy(y).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocessing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "mean = X.mean(0, keepdim=True)\n",
    "dev = X.std(0, keepdim=True)\n",
    "mean[:, 3] = 0. # the feature at column 3 is binary,\n",
    "dev[:, 3] = 1.  # so I'd rather not standardize it\n",
    "X = (X - mean) / dev\n",
    "X_test = (X_test - mean) / dev\n",
    "train = TensorDataset(X, y)\n",
    "test = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(test, batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(13, 32)\n",
    "        self.fc2 = nn.Linear(32, 24)\n",
    "        self.fc3 = nn.Linear(24, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 13)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Connect with remote workers</h2>\n",
    "<strong><me>Before this step, it is necessary to initialize the workers separately<me></strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = gr.WebsocketGridClient(hook, \"http://localhost:3000\", id=\"Bob\")\n",
    "alice =  gr.WebsocketGridClient(hook, \"http://localhost:3001\", id=\"Alice\")\n",
    "james = gr.WebsocketGridClient(hook, \"http://localhost:3002\", id=\"James\")\n",
    "bob.connect()\n",
    "alice.connect()\n",
    "james.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Ensure workers can communicate with each other</h2>\n",
    "For this to be possible, you need to introduce them to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob.connect_grid_node(alice.uri, alice.id)\n",
    "time.sleep(0.5)\n",
    "bob.connect_grid_node(james.uri, james.id)\n",
    "time.sleep(0.5)\n",
    "\n",
    "alice.connect_grid_node(james.uri, james.id)\n",
    "time.sleep(0.5)\n",
    "alice.connect_grid_node(bob.uri, bob.id)\n",
    "time.sleep(0.5)\n",
    "\n",
    "james.connect_grid_node(alice.uri, alice.id)\n",
    "time.sleep(0.5)\n",
    "james.connect_grid_node(bob.uri, bob.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Populate workers with dataset and define training/test routines</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_nodes = [bob, alice]\n",
    "\n",
    "\n",
    "train_distributed_dataset = []\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    target = target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    train_distributed_dataset.append((data, target))\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(train_distributed_dataset):\n",
    "        worker = data.location\n",
    "        model.send(worker)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # update the model\n",
    "        pred = model(data)\n",
    "        loss = F.mse_loss(pred.view(-1), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * data.shape[0], len(train_loader),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        #data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.mse_loss(output.view(-1), target, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Perform Train/test routines and measure performance</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/51 (0%)]\tLoss: 499.737000\n",
      "Train Epoch: 1 [80/51 (20%)]\tLoss: 444.433746\n",
      "Train Epoch: 1 [160/51 (39%)]\tLoss: 332.781433\n",
      "Train Epoch: 1 [240/51 (59%)]\tLoss: 133.071625\n",
      "Train Epoch: 1 [320/51 (78%)]\tLoss: 202.246490\n",
      "Train Epoch: 1 [200/51 (98%)]\tLoss: 16.212305\n",
      "Total 138.32 s\n",
      "\n",
      "Test set: Average loss: 25.9796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "\n",
    "total_time = time.time() - t\n",
    "print('Total', round(total_time, 2), 's')\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
