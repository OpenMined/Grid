{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Host and Query Model with Grid</h1>\n",
    "<p>In this series of tutorials we show how you can serve and query models on Grid platform.</p>\n",
    "\n",
    "**NOTE:** At the time of running this notebook, we were running the grid components in background mode.  \n",
    "\n",
    "**NOTE:**\n",
    "Components:\n",
    " - Grid Gateway(http://localhost:5000)\n",
    " - Grid Node Alice (http://localhost:3000)\n",
    " - Grid Node Bob (http://localhost:3001)\n",
    " - Grid Node Bill (http://localhost:3002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import dependencies</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f49b2cda9b24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'helper'"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import grid as gr\n",
    "import grid.syft as sy\n",
    "\n",
    "# GPT-2 model\n",
    "from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Set up configs</h3>\n",
    "<p>Now, we'll set our syft hook and instantiate our grid</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_grid = gr.GridNetwork(\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = helper.read_skin_cancer_dataset()\n",
    "train_df, valid_df, test_df = helper.split_data(df)\n",
    "\n",
    "# These values are from Part 1.\n",
    "input_size = 32\n",
    "train_mean, train_std = (th.tensor([0.6979, 0.5445, 0.5735]), th.tensor([0.0959, 0.1187, 0.1365]))\n",
    "\n",
    "# Create a test dataloader\n",
    "test_set = helper.Dataset(test_df, transform=helper.transform(input_size, train_mean, train_std))\n",
    "test_generator = th.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "\n",
    "# Get a data sample and a target\n",
    "data, target = next(iter(test_generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace model\n",
    "model = helper.make_model()\n",
    "# model.load_state_dict(torch.load(\"binary-skin-cancer-detection-model\"))\n",
    "model.eval()\n",
    "traced_model = th.jit.trace(model, data)\n",
    "\n",
    "# Plan model\n",
    "plan_model = helper.make_model(is_plan=True)\n",
    "# model.load_state_dict(torch.load(\"binary-skin-cancer-detection-model\"))\n",
    "plan_model.eval()\n",
    "plan_model.build(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Hosting our models</h3>\n",
    "<p>Now,we'll host our models on grid nodes registered on our grid network. For test purposes, we'll register the same model (can be jit model or plan model) changing only the model_id. </p>  \n",
    "<p>Nowadays, the choose of grid node that will host the model is random, but in the future we can add a better metric to do this.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_grid.serve_model(traced_model, \"skin-cancer-grid-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Querying/Running inference</h3>\n",
    "<p>Now, we want to recover the reference of our models registered previously.</p>\n",
    "<p>It is important to notice that we can have the same ID on different nodes (we can have different versions of same model distributed throughout our network). To solve this problem, when we perform a query, it will return a list of grid nodes that contains some version of this specific model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_grid.run_remote_inference(model_id=\"skin-cancer-grid-model\", dataset=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Disconnect Nodes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_grid.disconnect_nodes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
