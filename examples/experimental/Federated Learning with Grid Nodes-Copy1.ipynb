{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Federated Learning with Grid Nodes </h1>\n",
    "This notebook was made based on <a href=\"https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/Part%2010%20-%20Federated%20Learning%20with%20Secure%20Aggregation.ipynb\">Part 10: Federated Learning with Encrypted Gradient Aggregation</a> tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import dependencies</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import grid as gr\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import syft as sy\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Setup config</h2>\n",
    "Define Config parameters, init hook, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "class Parser:\n",
    "    \"\"\"Parameters for training\"\"\"\n",
    "    def __init__(self):\n",
    "        self.epochs = 1\n",
    "        self.lr = 0.001\n",
    "        self.test_batch_size = 8\n",
    "        self.batch_size = 8\n",
    "        self.log_interval = 10\n",
    "        self.seed = 1\n",
    "    \n",
    "args = Parser()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Grid nodes publish datasets online and are for EXPERIMENTAL use only.Deploy nodes at your own risk. Do not use OpenGrid with any data/models you wish to keep private.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alice =  gr.WebsocketGridClient(hook, \"http://localhost:3001\", id=\"Alice\")\n",
    "alice.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.]], grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[0.]], requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:engineio.client:WebSocket connection was closed, aborting\n",
      "WARNING:engineio.client:WebSocket connection was closed, aborting\n",
      "WARNING:engineio.client:WebSocket connection was closed, aborting\n",
      "WARNING:engineio.client:WebSocket connection was closed, aborting\n",
      "WARNING:engineio.client:WebSocket connection was closed, aborting\n",
      "WARNING:engineio.client:WebSocket connection was closed, aborting\n",
      "WARNING:engineio.client:WebSocket connection was closed, aborting\n",
      "WARNING:engineio.client:WebSocket connection was closed, aborting\n",
      "WARNING:engineio.client:WebSocket connection was closed, aborting\n",
      "WARNING:engineio.client:WebSocket connection was closed, aborting\n",
      "WARNING:engineio.client:WebSocket connection was closed, aborting\n",
      "ERROR:engineio.client:packet queue is empty, aborting\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 1)\n",
    "\n",
    "    @sy.method2plan\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "\n",
    "# Create toy data\n",
    "x = torch.zeros(1, 1, 28, 28)\n",
    "y = torch.tensor([1.0])\n",
    "local_res = net(x)\n",
    "assert not net.forward.is_built\n",
    "\n",
    "net.forward.build(x)\n",
    "\n",
    "plan_ptr = net.send(alice)\n",
    "x_ptr = x.send(alice)\n",
    "remote_res = plan_ptr(x_ptr).get()\n",
    "print(local_res)\n",
    "print(remote_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Connect with remote workers</h2>\n",
    "<strong><me>Before this step, it is necessary to initialize the workers separately<me></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Ensure workers can communicate with each other</h2>\n",
    "For this to be possible, you need to introduce them to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.connect_all_nodes((bob, alice, james))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Populate workers with dataset and define training/test routines</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_nodes = [bob, alice]\n",
    "\n",
    "\n",
    "train_distributed_dataset = []\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    target = target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    train_distributed_dataset.append((data, target))\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(train_distributed_dataset):\n",
    "        worker = data.location\n",
    "        model.send(worker)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # update the model\n",
    "        pred = model(data)\n",
    "        loss = F.mse_loss(pred.view(-1), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * data.shape[0], len(train_loader),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        #data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.mse_loss(output.view(-1), target, reduction='sum').item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Perform Train/test routines and measure performance</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/51 (0%)]\tLoss: 499.737000\n",
      "Train Epoch: 1 [80/51 (20%)]\tLoss: 444.433746\n",
      "Train Epoch: 1 [160/51 (39%)]\tLoss: 332.781433\n",
      "Train Epoch: 1 [240/51 (59%)]\tLoss: 133.071625\n",
      "Train Epoch: 1 [320/51 (78%)]\tLoss: 202.246490\n",
      "Train Epoch: 1 [200/51 (98%)]\tLoss: 16.212305\n",
      "Total 138.32 s\n",
      "\n",
      "Test set: Average loss: 25.9796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "\n",
    "total_time = time.time() - t\n",
    "print('Total', round(total_time, 2), 's')\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
