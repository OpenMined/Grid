{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Quickstart Using Docker Compose to Locally Run PyGrid\n",
    "\n",
    "In this tutorial, you'll learn how to use Docker to deploy a PyGrid Gateway and 4 Nodes into a local machine and then interact with it using PySyft. This quickly gets you up and running to test out the functionality of PySyft with the additonal management power of PyGrid. \n",
    "\n",
    "This method would not be used to setup a production environment. Instead, you would deploy the Gateway to a cloud server, and each Node to a separate client (ie. phone, hospital, etc). See the next tutorial for that: Part 2 - Deploying PyGrid Using the CLI.\n",
    "\n",
    "_WARNING: Grid nodes publish datasets online and are for EXPERIMENTAL use only. Deploy nodes at your own risk. Do not use OpenGrid with any data/models you wish to keep private._\n",
    "\n",
    "\n",
    "### Step 1: Download the repository\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/OpenMined/PyGrid/\n",
    "```\n",
    "\n",
    "### Step 2: Download dependencies\n",
    "\n",
    "You'll need to have the app dependencies installed. Use Python 3.6 or 3.7. We recommend setting up an independent [conda environment](https://docs.conda.io/projects/conda/en/latest/user-guide/concepts/environments.html) to avoid problems with library versions.\n",
    "\n",
    "You can install the dependencies by running:\n",
    "\n",
    "```bash\n",
    "cd PyGrid\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Step 3: Start Docker Containers\n",
    "\n",
    "First, [install Docker](https://docs.docker.com/get-docker/) if it isn't already installed on your computer.\n",
    "\n",
    "This tutorial is composed of 6 Docker containers: 1 Grid Gateway, 1 Redis database, and 4 Grid Nodes. You can think of it like each service could be run on a different computer. Technically the Redis container would be run on each computer that hosts a Grid Node. For the ease of this tutorial, we'll run all the services on your computer over localhost.\n",
    "    \n",
    "To easily get all these services up, we use Docker Compose. From the Docker documentation:\n",
    "\n",
    "> Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your applicationâ€™s services. Then, with a single command, you create and start all the services from your configuration.\n",
    "\n",
    "Powerful stuff. The file `PyGrid/docker-compose.yml` specifies the configuration. If you're unclear on what Docker Compose does, give this file a look and it should be more clear what's going on.\n",
    "\n",
    "To actually start the Docker containers, run the following in the same directory as `docker-compose.yml`:\n",
    "\n",
    "```bash\n",
    "docker-compose up\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Creating a Grid Worker and start communication\n",
    "\n",
    "First, import the required libraries. Then, hook PyTorch with PySyft to extend the functionality of PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import syft as sy\n",
    "from syft.workers.node_client import NodeClient\n",
    "from syft.grid.public_grid import PublicGridNetwork\n",
    "\n",
    "hook = sy.TorchHook(th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to a Node worker called Bob with WebSockets. In a production environment, this could be a phone, hospital, etc. \n",
    "\n",
    "***WARNING:*** Make sure to use the same address/port as the one used to start the worker! The code as it is will work if you're using docker-compose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bob'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob = NodeClient(hook, \"ws://localhost:3000\")\n",
    "\n",
    "bob.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a Torch tensor with toy data. Then, send the data to Bob and get back a pointer to x, which points to the data that is now on Bob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[PointerTensor | me:61027395165 -> Bob:11556638281]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = th.tensor([1,2,3,4])\n",
    "x = data.send(bob)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're finally at the heart of what makes PySyft useful: computation. We're going to perform a remote computation on the data we sent to Bob, without having the data in this notebook. \n",
    "\n",
    "Note that we're going to see a pointer tensor instead of a numeric result. This is expected. We'll get the result back in the next step. You can imagine that many more computations would typically be run on the data before we get back the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[PointerTensor | me:39529581238 -> Bob:63140690368]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + x\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the numeric result of the remote computation from Bob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 6, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.get()\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Start communication with the Grid Gateway and workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the Grid Gateway running on Docker. It allows us to interact with the Node workers we also started with Docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gateway = PublicGridNetwork(hook,\"http://localhost:5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the worker nodes.\n",
    "\n",
    "***WARNING:*** Make sure to use the same address/port as the one used to start the worker! The code as it is will work if you're using docker-compose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = NodeClient(hook, \"ws://localhost:3000\")\n",
    "alice = NodeClient(hook, \"ws://localhost:3001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send some toy data to Bob, and tag it with #tensor to be searchable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[PointerTensor | me:16721743307 -> Bob:48327504592]\n",
       "\tTags: #tensor \n",
       "\tShape: torch.Size([5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = th.tensor([1, 2, 3, 4, 5]).tag(\"#tensor\").send(bob)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the gateway searches for any datasets tagged #tensor across ALL connected workers. It should show the dataset we just sent to Bob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bob': [(Wrapper)>[PointerTensor | me:37705196978 -> Bob:48327504592]\n",
       "  \tTags: #tensor \n",
       "  \tShape: torch.Size([5])]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gateway.search(\"#tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also send some toy data to Alice, and tag it with #tensor to be searchable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[PointerTensor | me:61000211801 -> Alice:52617331829]\n",
       "\tTags: #tensor \n",
       "\tShape: torch.Size([5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = th.tensor([5, 4, 3, 2, 1]).tag(\"#tensor\").send(alice)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gateway should now show the datasets we sent to Bob and to Alice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bob': [(Wrapper)>[PointerTensor | me:69104242676 -> Bob:48327504592]\n",
       "  \tTags: #tensor \n",
       "  \tShape: torch.Size([5])],\n",
       " 'Alice': [(Wrapper)>[PointerTensor | me:3231242668 -> Alice:52617331829]\n",
       "  \tTags: #tensor \n",
       "  \tShape: torch.Size([5])]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = gateway.search(\"#tensor\")\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "We can use the gateway to do all kind of jobs. We saw it already search for datasets on workers by tags.\n",
    "\n",
    "We can also use the gateway to do things like querying workers, serving models (either encrypted or unencrypted), and running remote inferences.\n",
    "\n",
    "In the next tutorial, we'll see how we can launch these services one-at-a-time, instead of all-at-once with Docker Compose. This will get us towards being able to deploy a gateway and nodes for an actual application!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gridEnv]",
   "language": "python",
   "name": "conda-env-gridEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
